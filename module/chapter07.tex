\chapter{Data Cleaning and Preparation}

\section{Introduction}

Data cleaning and preparation are fundamental steps in any data analysis process. High-quality and well-prepared data ensures that subsequent analysis, modelling, and decision-making are accurate and reliable. Without appropriate cleaning and preparation, even advanced analytics and visualisations may produce misleading insights \cite{kim2020data}. 

Data cleaning involves identifying and correcting errors or inconsistencies within the dataset, such as missing values, outliers, or duplicate records \cite{rahm2000data}. Meanwhile, data preparation encompasses broader activities including transforming, integrating, and organising data into a usable form for analysis \cite{kotu2014predictive}. 

In business environments, these processes are critical because data often comes from multiple sources with differing formats, structures, and quality levels. For management and decision-makers, understanding the importance of data cleaning and preparation helps in appreciating the time and effort required before data can provide meaningful insights \cite{gandomi2015beyond}. This chapter introduces the key concepts, techniques, and challenges in cleaning and preparing data for effective analysis and strategic business decisions.

\section{Understanding Data Quality}

\subsection{Definition and Importance}

Data quality refers to the degree to which data is accurate, complete, reliable, and relevant for its intended purpose \cite{wang1996beyond}. In a business context, high-quality data enables effective decision-making, operational efficiency, and strategic planning. Conversely, poor data quality can lead to incorrect analyses, flawed business decisions, and financial losses \cite{redman1998impact}.

Key dimensions of data quality include accuracy, completeness, consistency, timeliness, and validity \cite{pipino2002data}. Accuracy indicates whether data correctly describes the real-world objects it represents. Completeness refers to the extent to which all required data is present. Consistency ensures that data is the same across different datasets or systems. Timeliness assesses whether data is up to date, while validity relates to data conforming to defined formats or rules.

Understanding data quality is critical for management students, as it highlights the risks associated with relying on data without assessing its quality first \cite{strong1997data}. For example, marketing campaigns based on inaccurate customer information may fail to reach their intended audience, resulting in wasted resources and missed opportunities. Therefore, awareness of data quality principles helps business leaders ask the right questions when presented with analytical reports or strategic recommendations derived from data.

\subsection{Dimensions of Data Quality}

Data quality is a multi-dimensional concept encompassing several attributes that determine whether data is fit for use. Understanding these dimensions helps organisations assess and improve their data effectively \cite{lee2006data}.

The main dimensions of data quality are:

\textbf{1. Accuracy.} This refers to how correctly data represents real-world values or events. For example, a customer's recorded date of birth should match their actual date of birth. Inaccurate data can lead to faulty analysis and poor decisions \cite{batini2009methodologies}.

\textbf{2. Completeness.} Completeness indicates whether all required data is available. Missing data can limit analysis scope. For instance, if income data is missing for many customers, segmentation or credit scoring models become unreliable.

\textbf{3. Consistency.} Consistency refers to data being the same across different datasets or systems. For example, if a customer's address is recorded differently in the sales and billing systems, operational errors may occur \cite{madnick2009overview}.

\textbf{4. Timeliness.} Timeliness ensures that data is up to date. Outdated data reduces relevance. For example, using last year's market data to make current pricing decisions could result in poor competitiveness.

\textbf{5. Validity.} Validity indicates whether data conforms to defined formats, rules, or standards. For example, a postal code field should contain only valid codes within the specified region.

\textbf{6. Uniqueness.} This dimension ensures that each entity is recorded once without duplication. Duplicate records can distort analysis results, such as inflating customer counts in reports.

For business managers, these dimensions highlight specific areas to examine when assessing data quality, supporting better governance and decision-making \cite{caballero2014quality}.


\section{Data Cleaning and Preparation Overview}

\subsection{Definition of Data Cleaning}

Data cleaning is the process of identifying and correcting errors, inconsistencies, or inaccuracies in datasets to improve their quality and reliability for analysis and decision-making \cite{kurgan2006survey}. This includes handling missing values, removing duplicate records, correcting typographical errors, and resolving data inconsistencies.

In business contexts, data cleaning ensures that information used for strategic planning, performance monitoring, and customer analysis is trustworthy. For example, if customer email addresses contain typos or outdated entries, marketing campaigns may fail to reach their intended audience, leading to wasted resources and missed opportunities \cite{rahm2000dataquality}.

Data cleaning is often considered one of the most time-consuming steps in data analysis projects, with studies showing it can take up to 80\% of a data analyst's or data scientist's time \cite{dasu2003exploratory}. However, this investment is crucial because decisions based on unclean data can result in costly mistakes, such as incorrect sales forecasts, misallocated budgets, or compliance risks due to reporting errors.

Ultimately, data cleaning lays the foundation for all subsequent data preparation, integration, and analytical tasks, enabling organisations to derive meaningful and reliable insights from their data assets.

\subsection{Definition of Data Preparation}

Data preparation is the process of transforming raw data into a form that is suitable for analysis and decision-making. It involves a series of steps including data cleaning, integration, transformation, reduction, and formatting to ensure the data is ready for use in analytical models or reporting \cite{pyle1999data}.

In business contexts, data preparation enables analysts to convert scattered, inconsistent, or unstructured data into structured datasets that can answer specific business questions effectively. For example, preparing sales data may involve merging records from multiple stores, standardising product codes, aggregating weekly sales, and reformatting dates into a consistent structure.

Effective data preparation improves the efficiency and accuracy of analytical tasks, supporting informed decision-making, performance monitoring, and strategic planning \cite{zeller2014data}.

\subsection{Relationship between Cleaning and Preparation}

Data cleaning and data preparation are closely related concepts but are not the same. Data cleaning is a **subprocess within data preparation** focused on identifying and correcting errors, inconsistencies, and inaccuracies in data \cite{kelleher2015fundamentals}. It ensures that the data is of high quality.

Data preparation, however, is a broader process. It includes data cleaning as well as other activities such as data integration (combining datasets from different sources), data transformation (changing data formats or structures to match analytical needs), data reduction (selecting relevant variables or aggregating data), and data formatting (ensuring data conforms to required input formats for models or tools) \cite{han2011data}.

In simple terms, **data cleaning ensures that data is correct and reliable, while data preparation ensures that data is ready to be used for its intended purpose**. Both are essential for accurate analysis and successful business decision-making.

\subsection{Key Data Cleaning Techniques}

Data cleaning involves various techniques to ensure that data is accurate, consistent, and ready for analysis. The choice of technique depends on the nature of the data and the types of issues identified \cite{rahm2000data}.

\textbf{1. Handling Missing Values.}  
Missing data is common in business datasets due to human error, system limitations, or incomplete processes. Techniques to address missing values include:

- \emph{Deletion}, where rows with missing values are removed if they are few and unlikely to bias the dataset.
- \emph{Imputation}, where missing values are replaced with estimated values such as the mean, median, mode, or values predicted by other variables.

For example, if customer age is missing in a dataset, it can be replaced with the average age of similar customers to avoid data loss.

\textbf{2. Handling Outliers.}  
Outliers are data points that differ significantly from other observations. They can result from data entry errors or represent rare but valid cases. Common techniques include:

- \emph{Investigation}, to determine whether outliers are errors or genuine extreme values.
- \emph{Removal or transformation}, where outliers that are errors are corrected or deleted, while valid outliers may be transformed using scaling or log transformation to reduce their impact on analysis \cite{barnett1994outliers}.

\textbf{3. Data Normalisation.}  
Normalisation scales numerical data to a common range, such as 0 to 1, to ensure that variables with larger scales do not dominate analytical models. Common methods include:

- \emph{Min-Max scaling}, where data is rescaled to a specified range.  
- \emph{Z-score standardisation}, where data is centred around the mean with a standard deviation of one \cite{jain2005score}.

\textbf{4. Removing Duplicates.}  
Duplicate records often arise when combining datasets from multiple sources. Removing them ensures each entity is only represented once, preventing overestimation in analyses such as customer counts or sales volumes.

\textbf{5. Correcting Data Entry Errors.}  
Typographical errors or inconsistencies in formats (e.g. date formats, product codes) are corrected by standardising entries to a consistent format or validating against reference data sources.

Applying these data cleaning techniques improves data quality and reliability, ensuring that business decisions are based on sound, accurate information \cite{oliveira2005data}.


\section{Data Integration}

\subsection{Definition and Purpose}

Data integration is the process of combining data from different sources to provide a unified, consistent view for analysis and decision-making \cite{lenzerini2002data}. It involves merging datasets that may have varying formats, structures, or definitions into a single dataset that can be used efficiently.

In business contexts, data integration is essential because organisations often store information across multiple systems. For example, customer data might be spread across sales databases, customer service records, and marketing platforms. Without integration, analysing the full customer journey or creating accurate segmentation becomes challenging \cite{doan2012principles}.

The main purposes of data integration are:

- \textbf{Holistic View.} Integration enables organisations to view and analyse all relevant data in one place, supporting comprehensive insights.
- \textbf{Improved Decision-Making.} By combining data from multiple sources, managers can make more informed decisions based on complete and consistent information.
- \textbf{Operational Efficiency.} Integrated data reduces duplication of work, streamlines processes, and improves communication between departments.
- \textbf{Data Consistency.} Integration ensures that data across systems is consistent, avoiding conflicting reports or decisions based on incomplete information.

For example, integrating inventory data from warehouse systems with sales data enables accurate demand forecasting and efficient stock management. Similarly, merging customer feedback with purchase history helps identify service improvements and product development opportunities \cite{hernandez1995merge}.

Overall, data integration is a critical process in preparing data for analysis, enabling organisations to leverage their data assets effectively for strategic and operational goals.

\subsection{Techniques for Data Integration}

Various techniques are used to integrate data from multiple sources effectively. The choice of technique depends on the nature of the data, the systems involved, and the business objectives \cite{singh2005survey}.

\textbf{1. ETL (Extract, Transform, Load).}  
This is the most common integration approach where data is:

- \emph{Extracted} from different source systems.  
- \emph{Transformed} into a common format or structure.  
- \emph{Loaded} into a target system, such as a data warehouse.

For example, sales data from regional branches is extracted, transformed into a standard currency and format, and then loaded into the company’s central reporting database \cite{vassiliadis2002etl}.

\textbf{2. ELT (Extract, Load, Transform).}  
In this approach, data is first extracted and loaded into the target system (such as a data lake) and then transformed within that system. This is suitable when the target has high processing power and is often used in big data environments.

\textbf{3. Data Federation (Virtual Integration).}  
Instead of physically moving data, this technique uses a virtual layer to integrate data from different sources in real-time. Queries are run across multiple databases, presenting results as if the data were integrated. This is useful for situations requiring up-to-date data without duplicating storage \cite{halevy2001answering}.

\textbf{4. Data Warehousing.}  
A data warehouse consolidates data from various sources into a central repository designed for analysis and reporting. Data is cleaned, transformed, and stored in a structured format, enabling efficient querying for business intelligence purposes.

\textbf{5. Application-Based Integration.}  
In this approach, integration is achieved through applications that communicate directly via APIs or integration software. For example, integrating CRM and ERP systems so that customer orders in CRM automatically update inventory levels in ERP.

\textbf{6. Manual Integration.}  
Sometimes, integration is performed manually, especially in small-scale projects or when dealing with ad-hoc datasets, using tools like spreadsheets to merge and align data. However, this is prone to human error and is not scalable for enterprise needs.

Each technique has its advantages and limitations. ETL is effective for structured data integration, while data federation is ideal for real-time needs without data duplication. Understanding these techniques enables managers to collaborate effectively with IT teams in planning data projects that support strategic goals \cite{hasselbring2000information}.

\subsection{Challenges in Data Integration}

While data integration offers significant benefits, it also presents various challenges that organisations must address to ensure successful implementation \cite{alexe2006cleaning}.

\textbf{1. Data Heterogeneity.}  
Data often comes from different sources with varying formats, structures, and standards. For example, customer data in one system may record dates as “DD/MM/YYYY” while another uses “MM-DD-YYYY”. Harmonising such differences requires careful transformation and standardisation to avoid errors in analysis.

\textbf{2. Schema and Semantic Differences.}  
Different systems may use different naming conventions and meanings for similar data. For instance, one database might label a field as “CustomerID” while another uses “CustNum”. Additionally, terms like “sales” may refer to gross sales in one dataset and net sales in another, creating potential confusion and integration errors \cite{doan2003reconciling}.

\textbf{3. Data Quality Issues.}  
Integration can magnify existing data quality problems, such as missing values, duplicates, or inaccuracies. Combining such data without cleaning can lead to unreliable outputs and flawed business decisions.

\textbf{4. Scalability and Performance.}  
Integrating large volumes of data, especially in real-time, can strain system resources. Ensuring that integration processes are scalable and do not slow down operations is a major technical challenge \cite{nash2019big}.

\textbf{5. Security and Privacy Concerns.}  
Integrating data often involves moving and accessing sensitive information across systems. Ensuring compliance with data privacy regulations (such as GDPR) and maintaining data security are critical considerations in integration projects \cite{zhang2019security}.

\textbf{6. Organisational and Governance Challenges.}  
Beyond technical issues, integrating data requires cooperation across departments, agreement on data definitions and ownership, and establishing data governance policies to maintain consistency and accountability.

Addressing these challenges requires a combination of technical solutions, clear organisational policies, and collaboration between business and IT teams to achieve reliable, efficient, and secure data integration that supports strategic goals.


\section{Ensuring Data Consistency}

\subsection{Definition and Importance}

Data consistency refers to the uniformity and coherence of data across different datasets, systems, or processes within an organisation. It ensures that data values do not conflict between systems and remain accurate and reliable over time \cite{chen2012data}.

For example, if a customer’s address is updated in the sales database but not in the shipping system, inconsistencies arise, potentially leading to delivery errors and poor customer satisfaction. Similarly, in financial reporting, if revenue figures differ between accounting and sales databases due to inconsistent updates, it may result in incorrect strategic decisions and compliance issues.

The importance of data consistency in business includes:

- \textbf{Improved Decision-Making.} Consistent data ensures that management decisions are based on uniform and reliable information, reducing risks associated with conflicting data.

- \textbf{Operational Efficiency.} Processes such as order fulfilment, billing, and customer service depend on consistent data to operate smoothly without delays or errors.

- \textbf{Regulatory Compliance.} Many industries require consistent reporting across systems to meet legal and regulatory standards. Inconsistencies can result in compliance breaches and financial penalties \cite{rahm2000dataquality}.

- \textbf{Enhanced Customer Experience.} Consistent customer data across touchpoints ensures that communications and services are personalised and accurate, improving satisfaction and loyalty.

Maintaining data consistency requires effective data governance policies, integration processes, and regular validation to ensure all systems reflect the same, correct data \cite{ottoo2013data}.

\subsection{Methods to Ensure Consistency}

Ensuring data consistency requires implementing systematic methods and organisational practices to maintain uniform data across systems and processes. Key methods include \cite{watson2009data}:

\textbf{1. Master Data Management (MDM).}  
MDM involves creating a single, authoritative source for critical business data such as customer, product, or supplier information. It ensures that all departments and systems refer to the same master records, reducing duplication and conflicting data \cite{otto2011mdm}.

\textbf{2. Data Validation Rules.}  
Applying validation rules during data entry and integration ensures that data meets required standards before it enters systems. For example, enforcing consistent date formats or valid product codes across datasets prevents inconsistencies.

\textbf{3. Referential Integrity Constraints.}  
In relational databases, referential integrity ensures that relationships between tables remain consistent. For instance, every order record referencing a customer ID must match an existing customer record. This prevents orphaned records or invalid references.

\textbf{4. Data Synchronisation.}  
Synchronising data between systems ensures updates made in one system are reflected in others. Techniques include batch updates, real-time synchronisation via APIs, or middleware integration tools.

\textbf{5. Regular Data Audits and Reconciliation.}  
Conducting periodic data audits identifies inconsistencies across systems. Reconciliation processes compare data between sources to detect and correct mismatches, such as financial transactions not matching across accounting and sales systems \cite{laurila1999data}.

\textbf{6. Clear Data Governance Policies.}  
Establishing policies defining data ownership, update responsibilities, and change management procedures ensures accountability and consistency in data handling across the organisation.

Implementing these methods improves data reliability, supports accurate reporting, and enhances organisational efficiency by ensuring all departments work with consistent and trustworthy information.

\subsection{Implications of Inconsistent Data}

Inconsistent data can have serious implications for organisations, affecting decision-making, operational efficiency, compliance, and customer satisfaction \cite{redman1996impact}.

\textbf{1. Poor Decision-Making.}  
Inconsistent data leads to conflicting reports and unreliable analysis results. For example, if sales figures differ between the finance and sales departments, management may make incorrect decisions regarding budgeting, inventory procurement, or market strategy.

\textbf{2. Reduced Operational Efficiency.}  
When different systems hold inconsistent data, employees may spend significant time reconciling and verifying information before completing tasks, slowing down processes and increasing operational costs \cite{madnick2009overview}.

\textbf{3. Compliance Risks.}  
Many industries require accurate and consistent data reporting for legal and regulatory compliance. Inconsistencies can result in misreporting, legal penalties, and damage to organisational reputation \cite{wang2006data}.

\textbf{4. Customer Dissatisfaction.}  
Inconsistent customer data across systems (e.g., outdated addresses or contact details) can lead to failed deliveries, incorrect billing, or poor service personalisation, reducing customer trust and loyalty.

\textbf{5. Increased Costs.}  
Resolving inconsistencies after they occur often requires significant resources, including manual data cleaning, system updates, and process redesigns, resulting in increased operational costs \cite{pipino2002dataquality}.

\textbf{6. Missed Opportunities.}  
When integrated data is unreliable, organisations may miss market trends, customer insights, or operational improvements that could have provided competitive advantages.

Understanding these implications highlights why ensuring data consistency is critical for organisations seeking to make informed decisions, maintain efficiency, comply with regulations, and deliver high-quality services to customers.

\section{Challenges and Limitations}

\subsection{Organisational Challenges}

Beyond technical complexities, organisations often face several non-technical challenges when implementing data cleaning, preparation, and integration initiatives \cite{strong1997data}.

\textbf{1. Lack of Data Governance.}  
Many organisations lack formal data governance frameworks defining clear roles, responsibilities, and processes for managing data quality. Without governance, there is confusion over who owns data and who is responsible for its accuracy, leading to inconsistent practices across departments \cite{otto2011data}.

\textbf{2. Siloed Data Ownership.}  
Data is often controlled by different departments with varying priorities and standards. For example, the sales team may maintain customer data differently from the finance team, making integration and cleaning efforts difficult due to resistance to change or lack of collaboration \cite{khatri2010data}.

\textbf{3. Limited Awareness of Data Quality Importance.}  
Employees and managers may underestimate the impact of poor data quality on business outcomes, resulting in low motivation to participate in cleaning and preparation initiatives. This cultural barrier can undermine data projects despite technical readiness.

\textbf{4. Resource Constraints.}  
Effective data cleaning and preparation require skilled personnel, time, and financial investment. Organisations may prioritise immediate operational needs over long-term data quality improvement, resulting in under-resourced initiatives \cite{madnick2009overview}.

\textbf{5. Change Management Challenges.}  
Introducing new data standards, cleaning processes, or integration systems often requires changes in workflows, software use, and staff responsibilities. Resistance to change is common if benefits are not clearly communicated or if training is insufficient.

\textbf{6. Lack of Strategic Alignment.}  
Data initiatives may not align with overall business strategy. Without clear links to strategic objectives, data cleaning and preparation efforts may lack executive support, leading to fragmented implementation and limited long-term impact.

Addressing these organisational challenges requires strong leadership commitment, clear communication of data value, cross-department collaboration, and alignment of data initiatives with business goals to ensure sustainable improvements in data quality and utilisation.

\subsection{Technical Limitations}

In addition to organisational challenges, data cleaning, preparation, and integration efforts often face several technical limitations \cite{schellhase2007data}.

\textbf{1. Data Quality Issues in Source Systems.}  
If the original data sources contain significant errors, missing values, or inconsistencies, cleaning and integration processes become more complex and time-consuming. Automated tools may struggle with unstructured errors, requiring manual review.

\textbf{2. Lack of Standardisation Across Systems.}  
Different systems may store data using varying formats, coding schemes, and database structures. For example, product categories coded differently across branches require extensive mapping before integration, increasing project complexity \cite{batini2009methodologies}.

\textbf{3. System Compatibility Constraints.}  
Legacy systems or outdated databases may not support modern integration tools or APIs, limiting the ability to automate extraction, transformation, and loading processes efficiently.

\textbf{4. Scalability Challenges.}  
Processing and integrating large volumes of data, especially in real-time, require significant computational resources. Systems without sufficient scalability capabilities may experience performance bottlenecks, delaying decision-making or operational processes \cite{stonebraker2010sql}.

\textbf{5. Data Security and Privacy Risks.}  
Technical limitations in encryption, access controls, or secure integration pipelines may expose sensitive data to breaches during integration or cleaning processes, especially when data is moved between systems or combined with external sources \cite{zhang2019security}.

\textbf{6. Limited Tool Capabilities.}  
Some data cleaning or integration tools may not support specific business requirements, such as advanced data matching, semantic reconciliation, or processing unstructured data (e.g., emails, PDFs), requiring expensive custom development.

\textbf{7. Maintenance Complexity.}  
Once integration processes are built, maintaining them becomes challenging as data sources, structures, and business requirements change over time, requiring ongoing updates, testing, and monitoring.

Understanding these technical limitations enables organisations to plan realistic data projects, allocate appropriate resources, and select suitable tools and architectures to support sustainable data cleaning, preparation, and integration initiatives.


\section{Case Study}

\subsection{Overview}

\textbf{Case: Customer Data Integration at RetailCo}

RetailCo is a mid-sized retail company with physical stores across several cities and an online e-commerce platform. Over the years, customer data has been collected and stored in different systems:

\begin{itemize}
	\item The point-of-sale (POS) system stores customer purchase records at physical stores.
	\item The e-commerce platform records online purchase history, customer reviews, and delivery details.
	\item The customer loyalty programme stores membership information, accumulated points, and rewards redeemed.
\end{itemize}

Management wants to launch a personalised marketing campaign targeting high-value customers across all channels. However, they discover inconsistencies, duplicate records, and missing values when attempting to combine data from these systems.

Key issues identified:

\begin{enumerate}
	\item Customer names are spelled differently across systems (e.g. ``John Smith" vs. ``Jon Smith").
	\item The e-commerce platform records date of birth, but the POS system does not.
	\item Some email addresses in the loyalty programme database are outdated or missing.
	\item Customer IDs are system-specific with no unified identifier.
\end{enumerate}

\subsection{Analysis}

RetailCo's situation highlights common challenges in data cleaning, preparation, and integration:

\begin{enumerate}
	\item \textbf{Data Cleaning Needs.}
	\begin{itemize}
		\item Duplicate records for the same customer under different name spellings require deduplication and standardisation.
	\end{itemize}
	
	\item \textbf{Preparation Gaps.}
	\begin{itemize}
		\item Missing date of birth data in the POS system limits demographic segmentation.
		\item Possible solutions include collecting this data at physical stores or inferring missing demographics based on purchase patterns.
	\end{itemize}
	
	\item \textbf{Integration Challenges.}
	\begin{itemize}
		\item Different customer ID systems prevent direct merging of records.
		\item Implementation of a master data management (MDM) system is needed to unify customer identifiers.
	\end{itemize}
	
	\item \textbf{Consistency Issues.}
	\begin{itemize}
		\item Outdated or inconsistent email addresses reduce the effectiveness of marketing campaigns.
		\item Data validation and updating processes are necessary to maintain data relevance.
	\end{itemize}
	
	\item \textbf{Business Impact.}
	\begin{itemize}
		\item Ineffective marketing targeting.
		\item Customer dissatisfaction due to irrelevant offers.
		\item Missed revenue opportunities from cross-channel promotions.
	\end{itemize}
\end{enumerate}

Addressing these challenges would involve:

\begin{enumerate}
	\item Applying data cleaning techniques such as standardising name formats and deduplicating records.
	\item Integrating data using ETL processes combined with an MDM system to unify customer identifiers.
	\item Establishing data governance policies to maintain data quality across systems.
\end{enumerate}

This case illustrates the importance of data cleaning and preparation as prerequisites for successful data integration and strategic business initiatives.


\section{Hands-on Activity: Evaluating and Preparing Data Without Coding}

\textbf{Objective.}  
To practice data evaluation, cleaning, preparation, and integration planning using a real-world open dataset with guided tasks and expected solutions.

\textbf{Dataset.}

Use the \textbf{Online Retail dataset}:

\begin{itemize}
	\item Download from: \url{https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx}
	\item Open the Excel file and review the data in the provided sheet.
\end{itemize}

\textbf{Instructions.}

Perform the following steps individually:

\begin{enumerate}
	\item \textbf{Identify Data Quality Issues.}
	
	Review the dataset and note:
	
	\begin{enumerate}
		\item \textbf{Missing Values.} Filter the CustomerID column to find any blank entries.
		\item \textbf{Inconsistent Description Entries.} Check for product descriptions recorded in all uppercase vs. lowercase or mixed case formats.
		\item \textbf{Negative Quantities.} Sort Quantity to identify negative values indicating product returns.
	\end{enumerate}
	
	\textbf{Expected Solutions:}
	
	\begin{itemize}
		\item For missing CustomerIDs: propose either removing such rows if analysis requires customer segmentation or treating them as anonymous customers.
		\item For inconsistent descriptions: standardise to title case for consistency.
		\item For negative quantities: classify them as returns and create a flag column to indicate returned items.
	\end{itemize}
	
	\item \textbf{Prepare Data for Analysis.}
	
	\begin{enumerate}
		\item Create a new column categorising UnitPrice:
		\begin{itemize}
			\item UnitPrice < £1: “Low Price”
			\item UnitPrice £1–£10: “Medium Price”
			\item UnitPrice > £10: “High Price”
		\end{itemize}
		\item Reformat InvoiceDate to extract only the month for monthly sales trend analysis.
	\end{enumerate}
	
	\textbf{Expected Solutions:}
	
	\begin{itemize}
		\item Use Excel IF formulas to categorise UnitPrice values.
		\item Use Excel MONTH or TEXT functions to extract month from InvoiceDate.
	\end{itemize}
	
	\item \textbf{Integration Planning.}
	
	\begin{enumerate}
		\item Imagine a second dataset with customer demographic data containing CustomerID, AgeGroup, and LoyaltyStatus.
		\item Propose an integration plan to combine this with the Online Retail dataset for customer segmentation analysis.
	\end{enumerate}
	
	\textbf{Expected Solutions:}
	
	\begin{itemize}
		\item Perform a left join using CustomerID as the key to add demographic information to each transaction record.
	\end{itemize}
	
	\item \textbf{Reflection.}
	
	Write short answers to:
	
	\begin{enumerate}
		\item Why is data consistency important when analysing sales and customer behaviour data?
		\item What organisational or technical challenges could arise in integrating transactional and demographic data?
	\end{enumerate}
	
	\textbf{Expected Solutions:}
	
	\begin{itemize}
		\item Consistency ensures analysis accuracy and reliable insights; inconsistency leads to wrong targeting or forecasts.
		\item Challenges include differing data ownership across departments, incompatible data formats, or privacy and access restrictions.
	\end{itemize}
	
	\item \textbf{Reporting.}
	
	\begin{itemize}
		\item Prepare a one-page report including:
		\begin{enumerate}
			\item Three data issues identified with cleaning proposals.
			\item Preparation steps with screenshots if applicable.
			\item Integration plan explanation.
			\item Reflection answers.
		\end{enumerate}
		\item Submit via the learning management system before the next class.
	\end{itemize}
\end{enumerate}

\textbf{Expected Outcome.}  
Students will build practical confidence in evaluating and preparing real-world business data for decision-making, using structured, guided steps without coding.


\section{Summary}

This chapter has introduced the concepts of data cleaning and preparation, emphasising their importance in ensuring that business data is accurate, complete, and ready for analysis. It explained key data cleaning techniques such as handling missing values, managing outliers, and normalising data, alongside the broader preparation processes including integration and ensuring consistency across systems. These foundational steps are critical to transforming raw data into reliable information that supports effective decision-making and strategic planning.

The chapter also discussed practical challenges in implementing data cleaning and integration initiatives, highlighting both organisational issues such as data ownership and governance, and technical limitations including system compatibility and scalability constraints. Through the hands-on activity using the Sample Superstore dataset, students gained practical experience evaluating data quality, planning cleaning steps, and integrating datasets without coding. Understanding these concepts equips future managers to lead data-driven projects confidently and collaborate effectively with technical teams to maximise business value from data assets.

